{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk+N29+zx18RU41iYWKRZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bogdan-Strat/NLP-with-disaster-tweets/blob/main/NLP_with_disaster_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install num2words\n",
        "! pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P06aQMfsBjMH",
        "outputId": "07615089-2069-4372-c112-32541df68ffc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234911 sha256=384df0fb3f7e7f39da2de26851a9f976be42648ffa1f014714d6a0efdaeccfc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from num2words import num2words\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "import emoji\n",
        "import string"
      ],
      "metadata": {
        "id": "JCEUN-ta93gb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z5bxY7uRfAy",
        "outputId": "58eccc8a-6e61-4e14-b440-f0ccc202b9b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      </?3                       # heart\n",
        "    )\"\"\""
      ],
      "metadata": {
        "id": "s9sGoz2_1dMT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Zm5l6D_vUvcc"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def lower_text(tweet):\n",
        "    return tweet.lower()\n",
        "\n",
        "def convert_number_to_words(tweet):\n",
        "  tweet_num2words = []\n",
        "  data = tweet.split()\n",
        "\n",
        "  for text in data:\n",
        "    if ',' in text:\n",
        "            parts = text.split(',')\n",
        "            converted_parts = []\n",
        "            for part in parts:\n",
        "                if part.isdigit():\n",
        "                    number_without_comma = int(part)\n",
        "                    converted_parts.append(num2words(number_without_comma))\n",
        "                else:\n",
        "                    converted_parts.append(part)\n",
        "            converted_word = ' '.join(converted_parts)\n",
        "            tweet_num2words.append(converted_word)\n",
        "    elif text.isdigit():\n",
        "      tweet_num2words.append(num2words(text))\n",
        "    else:\n",
        "      tweet_num2words.append(text)\n",
        "  return ' '.join(tweet_num2words)\n",
        "\n",
        "def remove_links(tweet):\n",
        "  return ' '.join([re.sub(r'http\\S+', '', word) for word in tweet.split(\" \")])\n",
        " \n",
        "def remove_emoticons_and_emojis(tweet):\n",
        "  emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
        "  tweet = ' '.join([re.sub(emoticon_re, '', word) for word in tweet.split(\" \")])\n",
        "  return emoji.replace_emoji(tweet, replace='')\n",
        " \n",
        "def remove_hashtags_and_mentions(tweet):\n",
        "  tweets_no_hashtags = [re.sub(r'#[a-zA-Z0-9_]+','', word) for word in tweet]\n",
        "  return ' '.join([re.sub(r'@[a-zA-Z0-9_]+','', word) for word in tweets_no_hashtags])\n",
        " \n",
        "def remove_multiple_spaces(tweet):\n",
        "  return  ' '.join([re.sub(r'\\s+', ' ', word).strip() for word in tweet.split(\" \")])\n",
        " \n",
        "'''\n",
        "Aveti grija la cazurile de tipul \"unu,doi\". Daca eliminati punctuatia direct, cele doua cuvinte vor \n",
        "fi concatenate obtinand un singur cuvant \"unudoi\". O alternativa ar fi sa inlocuim \n",
        "mai intai toate caracterele de punctuatie cu spatiu, apoi sa aplicam inca o data metoda de contractie a spatiilor.\n",
        "'''\n",
        " \n",
        "def remove_punctuation(tweet):\n",
        "  tweets_no_punct = [re.sub(r'[^\\w\\s]', ' ', word) for word in tweet.split(\" \")]\n",
        "  return ' '.join([re.sub(r'\\s+', ' ', tweet) for tweet in tweets_no_punct])\n",
        "\n",
        "def preprocess(tweet):\n",
        "  tweet = lower_text(tweet)\n",
        "  tweet = convert_number_to_words(tweet)\n",
        "  tweet = remove_links(tweet)\n",
        "  tweet = remove_emoticons_and_emojis(tweet)\n",
        "  tweet = remove_multiple_spaces(tweet)\n",
        "  tweet = remove_punctuation(tweet)\n",
        " \n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tweet):\n",
        "  stop_words_nltk = set(stopwords.words('english'))\n",
        "  all_words = [word for word in tweet]\n",
        "  all_words_without_stops = [word for word in all_words if word not in stop_words_nltk]\n",
        "  return all_words_without_stops\n",
        "\n",
        "def lematizer(tweet):\n",
        "  words_lemmatize = []\n",
        "  stemmer = SnowballStemmer(language='english')\n",
        "  for token in tweet:\n",
        "    words_lemmatize.append(stemmer.stem(token))\n",
        "  return words_lemmatize\n",
        "\n",
        "def tokenizer(tweet):\n",
        "  tweet = word_tokenize(tweet)\n",
        "  tweet = remove_stopwords(tweet)\n",
        "  tweet = lematizer(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "8fg87_zaztR0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"train.csv\")  # Assuming you have a CSV file with columns id, body, and label\n",
        "data\n",
        "\n",
        "# Split the data into features (email body) and labels (spam or not spam)\n",
        "X = data['text']\n",
        "y = data['target']\n",
        "tokenizer(X[3])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the email bodies with lowercase preprocessing\n",
        "vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenizer)\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = nb_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = (predictions == y_test).mean()\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_DmrzVlHuPO",
        "outputId": "7a822c2c-2b9a-4af6-c064-c8f57f34aab4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7918581746552856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zaa8-WvvNzOV",
        "outputId": "526cc4e5-5f5f-48f9-dc8a-06b8875bf3cc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin\t dev   lib32   mnt\t\t\t python-apt  srv    usr\n",
            "boot\t etc   lib64   NGC-DL-CONTAINER-LICENSE  root\t     sys    var\n",
            "content  home  libx32  opt\t\t\t run\t     tmp\n",
            "datalab  lib   media   proc\t\t\t sbin\t     tools\n"
          ]
        }
      ]
    }
  ]
}