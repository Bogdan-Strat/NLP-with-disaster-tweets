{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSOyvp2iL/cQH3kLryTuKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bogdan-Strat/NLP-with-disaster-tweets/blob/main/NLP_with_disaster_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "JCEUN-ta93gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P06aQMfsBjMH",
        "outputId": "caa8decb-132c-4aac-f045-89303e65631a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=42bfc04acd0e575a5791c350a272c8368ec4f0359c1b574897dba852ae878906\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from num2words import num2words\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ICrDNcbOBn5U"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z5bxY7uRfAy",
        "outputId": "d0bc7525-2673-4147-d445-9fff0b0f5b96"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Zm5l6D_vUvcc",
        "outputId": "93f3b1d2-1d19-42c0-e699-d0248da09f05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thirteen zero people receive #wildfires evacuation orders in california'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def lower_text(tweet):\n",
        "    return tweet.lower()\n",
        "\n",
        "def convert_number_to_words(tweet):\n",
        "  tweet_num2words = []\n",
        "  data = tweet.split()\n",
        "\n",
        "  for text in data:\n",
        "    if ',' in text:\n",
        "            parts = text.split(',')\n",
        "            converted_parts = []\n",
        "            for part in parts:\n",
        "                if part.isdigit():\n",
        "                    number_without_comma = int(part)\n",
        "                    converted_parts.append(num2words(number_without_comma))\n",
        "                else:\n",
        "                    converted_parts.append(part)\n",
        "            converted_word = ' '.join(converted_parts)\n",
        "            tweet_num2words.append(converted_word)\n",
        "    elif text.isdigit():\n",
        "      tweet_num2words.append(num2words(text))\n",
        "    else:\n",
        "      tweet_num2words.append(text)\n",
        "  return ' '.join(tweet_num2words)\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(tweet):\n",
        "  tweet = lower_text(tweet)\n",
        "  tweet = convert_number_to_words(tweet)\n",
        "  return tweet\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"train.csv\")  # Assuming you have a CSV file with columns id, body, and label\n",
        "data\n",
        "\n",
        "# Split the data into features (email body) and labels (spam or not spam)\n",
        "X = data['text']\n",
        "y = data['target']\n",
        "preprocess(X[3])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the email bodies with lowercase preprocessing\n",
        "vectorizer = CountVectorizer(preprocessor=preprocess)\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = nb_classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = (predictions == y_test).mean()\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_DmrzVlHuPO",
        "outputId": "9c6902cc-b5d1-4b4a-b74e-21467b26f448"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8003939592908733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zaa8-WvvNzOV",
        "outputId": "526cc4e5-5f5f-48f9-dc8a-06b8875bf3cc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin\t dev   lib32   mnt\t\t\t python-apt  srv    usr\n",
            "boot\t etc   lib64   NGC-DL-CONTAINER-LICENSE  root\t     sys    var\n",
            "content  home  libx32  opt\t\t\t run\t     tmp\n",
            "datalab  lib   media   proc\t\t\t sbin\t     tools\n"
          ]
        }
      ]
    }
  ]
}